{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK NE Chunker\n",
    "In this notebook we explore NLTK ne_chunk for Named Entity Recognition. We go on to implement our own Music Instruments recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a sentence\n",
    "sentence = \"Barack stands at the Oval inside White House\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Barack/NNP)\n",
      "  stands/VBZ\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (FACILITY Oval/NNP)\n",
      "  inside/IN\n",
      "  (FACILITY White/NNP House/NNP))\n"
     ]
    }
   ],
   "source": [
    "# NLTK ne_chunk works on part-of-speech tags\n",
    "chunked = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conll tags can be generated directly from the ne_tree returned by ne_chunk. we may use this functionality in future.\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "iob_tagged = tree2conlltags(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'NNP', 'B-GPE'), ('stands', 'VBZ', 'O'), ('at', 'IN', 'O'), ('the', 'DT', 'O'), ('Oval', 'NNP', 'B-FACILITY'), ('inside', 'IN', 'O'), ('White', 'NNP', 'B-FACILITY'), ('House', 'NNP', 'I-FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "print(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Barack/NNP)\n",
      "  stands/VBZ\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (FACILITY Oval/NNP)\n",
      "  inside/IN\n",
      "  (FACILITY White/NNP House/NNP))\n"
     ]
    }
   ],
   "source": [
    "ne_tree = conlltags2tree(iob_tagged)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train custom NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    604\n",
       "line1    604\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the wiki extracted data\n",
    "parent_dir = '/Users/saurabh/workspace/datasets/wikimusic/'\n",
    "\n",
    "# load instruments_line.csv\n",
    "mi_f_name = parent_dir + \"instruments_line1.csv\"\n",
    "mi_df = pd.read_csv(mi_f_name, delimiter='|')\n",
    "mi_df.head(2)\n",
    "mi_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused\n",
    "# adapted from https://nlpforhackers.io/named-entity-extraction/\n",
    "# to created conll type tags for our custom tag \"MUSIC\"\n",
    "def to_conll_iob(annotated_sentence):\n",
    "    \"\"\"\n",
    "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
    "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    \"\"\"\n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        (tag, word), ner = annotated_token\n",
    " \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = \"I-\" + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append(((tag, word), ner))\n",
    "    return proper_iob_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for annotating wiki extracted data in a format that ne_chunk would understand\n",
    "def annotate_data(row):\n",
    "    retval = []\n",
    "    title, line1 = row\n",
    "    title_tkns = word_tokenize(title.lower())\n",
    "    tkns = word_tokenize(line1)\n",
    "    indices = []\n",
    "    for idx, tkn in enumerate(tkns):\n",
    "        if tkn.lower() in title_tkns:\n",
    "            indices.append(idx)\n",
    "    tagged = pos_tag(tkns)\n",
    "    for idx, tagged_tokens in enumerate(tagged):\n",
    "        if idx not in indices:\n",
    "            retval.append((tagged_tokens, 'O'))\n",
    "        else:\n",
    "            retval.append((tagged_tokens, 'MUSIC'))\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>line1</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agung a tamlang</td>\n",
       "      <td>The Agung a Tamlang is a type of Philippine sl...</td>\n",
       "      <td>[((The, DT), O), ((Agung, NNP), MUSIC), ((a, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slit drum</td>\n",
       "      <td>A slit drum is a hollow percussion instrument</td>\n",
       "      <td>[((A, DT), O), ((slit, NN), MUSIC), ((drum, NN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                              line1  \\\n",
       "0  Agung a tamlang  The Agung a Tamlang is a type of Philippine sl...   \n",
       "1        Slit drum      A slit drum is a hollow percussion instrument   \n",
       "\n",
       "                                           annotated  \n",
       "0  [((The, DT), O), ((Agung, NNP), MUSIC), ((a, D...  \n",
       "1  [((A, DT), O), ((slit, NN), MUSIC), ((drum, NN...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos tagging\n",
    "mi_df[\"annotated\"] = mi_df.apply(annotate_data, axis=1)\n",
    "mi_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi_df[\"annotated\"] = mi_df[\"annotated\"].apply(to_conll_iob)\n",
    "# mi_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test set\n",
    "msk = np.random.rand(len(mi_df)) < 0.8\n",
    "train = mi_df[msk]\n",
    "test = mi_df[~msk]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://nlpforhackers.io/named-entity-extraction/\n",
    "# This is the custom trainer built on top of ChunkParserI provided by NLTK\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    " \n",
    " \n",
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://nlpforhackers.io/named-entity-extraction/\n",
    "# The features that are extracted by custom trainer\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    " \n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev-iob': previob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cutome NER\n",
    "chunker = NamedEntityChunker(train[\"annotated\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Is', 'VBZ'), 'O'),\n",
       " (('it', 'PRP'), 'MUSIC'),\n",
       " (('not', 'RB'), 'O'),\n",
       " (('a', 'DT'), 'O'),\n",
       " (('slit', 'NN'), 'MUSIC'),\n",
       " (('drum', 'NN'), 'MUSIC'),\n",
       " (('?', '.'), 'O')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one sentence\n",
    "chunker.parse(pos_tag(word_tokenize(\"Is it not a slit drum?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth = test[\"annotated\"].values.tolist()\n",
    "test_pred = [chunker.parse(pos_tag(word_tokenize(sent))) for sent in test[\"line1\"].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "0.7652173913043478\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy of custom NER Tagger\n",
    "count = 0\n",
    "for i in range(len(test)):\n",
    "    tags = test_pred[i]\n",
    "    true_tags = test_truth[i]\n",
    "    accurate = True\n",
    "    for tag, true_tag in zip(tags, true_tags):\n",
    "        if true_tag[1] == \"MUSIC\" and tag[1] != \"MUSIC\":\n",
    "            accurate = False\n",
    "    if accurate:\n",
    "        count += 1\n",
    "print(count)\n",
    "print(count/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Extraction\n",
    "under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sem.relextract import tree2semi_rel, semi_rel2reldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[], Tree('GPE', [('Barack', 'NNP')])], [[('stands', 'VBZ'), ('at', 'IN'), ('the', 'DT')], Tree('FACILITY', [('Oval', 'NNP')])], [[('inside', 'IN')], Tree('FACILITY', [('White', 'NNP'), ('House', 'NNP')])]]\n"
     ]
    }
   ],
   "source": [
    "pairs = tree2semi_rel(chunked)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<class 'str'>, {'lcon': '', 'subjclass': 'GPE', 'subjtext': 'Barack/NNP', 'subjsym': 'barack', 'filler': 'stands/VBZ at/IN the/DT', 'untagged_filler': 'stands at the', 'objclass': 'FACILITY', 'objtext': 'Oval/NNP', 'objsym': 'oval', 'rcon': 'inside/IN'})]\n"
     ]
    }
   ],
   "source": [
    "reldicts = semi_rel2reldict(pairs)\n",
    "print(reldicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sem.relextract import rtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'.*\\bat\\b.*')\n",
    "relfilter = lambda x: (x['subjclass'] == subjclass and\n",
    "                       len(x['filler'].split()) <= window and\n",
    "                       pattern.match(x['filler']) and\n",
    "                       x['objclass'] == objclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjclass = 'GPE'\n",
    "objclass = 'FACILITY'\n",
    "window = 5\n",
    "rels = list(filter(relfilter, reldicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<class 'str'>, {'lcon': '', 'subjclass': 'GPE', 'subjtext': 'Barack/NNP', 'subjsym': 'barack', 'filler': 'stands/VBZ at/IN the/DT', 'untagged_filler': 'stands at the', 'objclass': 'FACILITY', 'objtext': 'Oval/NNP', 'objsym': 'oval', 'rcon': 'inside/IN'})]\n"
     ]
    }
   ],
   "source": [
    "print(rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPE: 'Barack/NNP'] 'stands/VBZ at/IN the/DT' [FACILITY: 'Oval/NNP']\n"
     ]
    }
   ],
   "source": [
    "for rel in rels:\n",
    "    print(rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
